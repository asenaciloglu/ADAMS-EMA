{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wv1VdKfI_T2B"
   },
   "source": [
    "# Adams Mini Challenge 2 \n",
    "### Team = EMA\n",
    "### Group Member: Edanur Kahvecioglu, Melike Merdan, Asena Ciloglu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xCE0t6FJBBoM"
   },
   "source": [
    "# PACKAGE IMPORTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4XDGY-7G_T2R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Asena\n",
      "[nltk_data]     C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Asena\n",
      "[nltk_data]     C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Asena C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Asena\n",
      "[nltk_data]     C\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Library beatifulsoup4 handles html\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM,GRU, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQP79uBK_T2j"
   },
   "source": [
    "# DATA IMPORTING & VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwvipPWCBvcJ"
   },
   "source": [
    "Our aim for the assignment is to predict ratings of books based on the reviews by Amazon users. The data is from Amazon Book Reviews which includes 9 variables such as product id, product title, book reviews, review title etc. We have a large data set, containing 1099209 observations, each represents single book review of a user. In our analysis, we will be working on the reviews (review_body) and the ratings of each review (star_rating)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOnvVvGuIJXh"
   },
   "source": [
    "In our analysis, we used two models: RNN and logistic regression. The reason for that is that RNN, by its nature, promises better analysis for 'Natural Language Processing' as it helps us to analyze the interactions of embedding layers and to run recurrent neural network which it is better for capturing semantic meaning of the texts. Furthermore, logistic regression is easy to implement and does not require high computational power.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jKIg84s6_T2l",
    "outputId": "e1fb7057-5f55-4152-abf0-91d72f553f14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109921, 10)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"sample_data.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "colab_type": "code",
    "id": "4xdxJHn4_T2r",
    "outputId": "2c979811-d8c5-43c9-949e-216e7c39db96"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>634662</td>\n",
       "      <td>634663</td>\n",
       "      <td>0880801484</td>\n",
       "      <td>The 5000 Year Leap</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>236</td>\n",
       "      <td>N</td>\n",
       "      <td>Light on facts.......</td>\n",
       "      <td>Skousen has distorted some facts, and cherry-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049228</td>\n",
       "      <td>1049229</td>\n",
       "      <td>1888762144</td>\n",
       "      <td>Spanish Grammar for Independent Learners</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>Y</td>\n",
       "      <td>Great Book</td>\n",
       "      <td>An excellent tool.  It covers verb conjugation...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  product_id  \\\n",
       "0      634662        634663  0880801484   \n",
       "1     1049228       1049229  1888762144   \n",
       "\n",
       "                              product_title  star_rating  helpful_votes  \\\n",
       "0                        The 5000 Year Leap            1             67   \n",
       "1  Spanish Grammar for Independent Learners            5             13   \n",
       "\n",
       "   total_votes verified_purchase        review_headline  \\\n",
       "0          236                 N  Light on facts.......   \n",
       "1           15                 Y             Great Book   \n",
       "\n",
       "                                         review_body  \n",
       "0  Skousen has distorted some facts, and cherry-p...  \n",
       "1  An excellent tool.  It covers verb conjugation...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hWnVk65TDtuV"
   },
   "source": [
    "For the sake of practicality, we took a random sample of 100.000 observations from the data to perform our coding steps. Since the whole data (MC2.csv) itself was too time consuming for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CypDv_RTBqBN"
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.seed(123)\n",
    "# df = data.sample(frac=0.1, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6HCK3O8_T26"
   },
   "source": [
    "# TRAIN / TEST SPLIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14M2IGCK_T28"
   },
   "source": [
    "In order to prevent overfitting due to processing test and train data together, in the very beginning we splitted data into test and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yn3B-iRy_T29"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review_body'], df['star_rating'], test_size=0.25, random_state=111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CDamLjCLHE5G"
   },
   "source": [
    "To perform logistic regression in the next steps, we needed to save our train and test samples beforehands so that they will not get affected from the preprocessing steps for our NN model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XMzxKLmshusA"
   },
   "outputs": [],
   "source": [
    "y_test_log = y_test[:]\n",
    "X_test_log = X_test[:]\n",
    "y_train_log = y_train[:]\n",
    "X_train_log = X_train[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vgt26Afo_T3B"
   },
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kk1xGnU3NB3r"
   },
   "outputs": [],
   "source": [
    "# Due to technicality, we needed to substract one from our ratings.\n",
    "#Later, we converted our rating results to arrays for neural network processes.\n",
    "y_train = y_train - 1\n",
    "y_test = y_test - 1\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "06xWP9Oh_T3H",
    "outputId": "c7a5648e-849b-4ecc-d4ae-c93b2025d1f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x203e2f55358>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[\"star_rating\"].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYhXwPUh_-RT"
   },
   "source": [
    "As can be seen from the histogram of star-ratings, 70% of the comments have 5-star-rating, which is likely to be imbalanced. Therefore, we used undersampling to eliminate the imbalance if possible.\n",
    "\n",
    " For undersampling, we found that star rating 2 has the lowest number of observations among all. Thus, we adjusted the number of other ratings with respect to 2. After we ran both of the models with undersampled data, our loss increased tremendously. We concluded that undersampling decreases the number of observations dramatically, which could carry valuable information as an input for our models. Eventually we did not use the undersampling method for our final models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kTCSfb03_T3d"
   },
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9W_N8pDB_T3e"
   },
   "source": [
    "In order to process the reviews, we applied common methods such as case equalization, part of speech tagging (wordnet_pos) according to word classes. Additionally, we performed BeautifulSoup to eliminate html texts if any, removed non-alphabetic characters and stopwords. Lastly, we lemmatized the words to create a simpler word set. We kept those word as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9Z5WGZ9_T3f"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "caIeZEH-_T3_"
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "X_train = X_train.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "X_test = X_test.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-aoCe-5_T4D"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6z-2qYw_T4F"
   },
   "outputs": [],
   "source": [
    "def clean_sentences(df):\n",
    "    reviews = []\n",
    "\n",
    "    for sent in df:\n",
    "\n",
    "        #remove html content\n",
    "        review_text = BeautifulSoup(sent).get_text()\n",
    "        \n",
    "        #remove non-alphabetic characters\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text) \n",
    "       \n",
    "        #tokenize the sentences\n",
    "        words = word_tokenize(review_text.lower())\n",
    "  \n",
    "        #lemmatize each word to its lemma\n",
    "        lemma_words =[lemmatizer.lemmatize(i, get_wordnet_pos(i)) for i in words]\n",
    "    \n",
    "        reviews.append(lemma_words)\n",
    "        \n",
    "\n",
    "    return(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2b6ADOZPOJRj"
   },
   "source": [
    "Due to high computational cost, we saved our clean sentences with pickle and used the pickles in the rest of the analysis. The code below shows the original code we used to create the pickles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1pb7R7zAf_C"
   },
   "outputs": [],
   "source": [
    "#train_sentences = clean_sentences(X_train)\n",
    "#test_sentences = clean_sentences(X_test)\n",
    "\n",
    "#filehandler = open('train.pkl',\"wb\")\n",
    "#pickle.dump(train_sentences,filehandler)\n",
    "#filehandler.close()\n",
    "\n",
    "#filehandler_test = open('test.pkl',\"wb\")\n",
    "#pickle.dump(test_sentences,filehandler_test)\n",
    "#filehandler_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PwocF8X5dX8I"
   },
   "outputs": [],
   "source": [
    "with open('train.pkl','rb') as filehandler:\n",
    "    train_sentences = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TnosSPsRdhr7"
   },
   "outputs": [],
   "source": [
    "with open('test.pkl','rb') as filehandler_test:\n",
    "    test_sentences = pickle.load(filehandler_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9LIET7ZCOiS9"
   },
   "source": [
    "In order to obtain input matrices for neural network, the sentence sequences are needed to be filled with zeros until they reach the length of the longest sentence. (pad_sequences).In the beginning, we set our input matrix's length according to longest sentence in either train or test dataset, whichever is the longer. Due to computational time restrictions, we changed it to 200 (the average length is 162)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXUTqWR2_T4J"
   },
   "outputs": [],
   "source": [
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(train_sentences)\n",
    "seq = tokenizer_obj.texts_to_sequences(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2LyBVhM_T4L"
   },
   "outputs": [],
   "source": [
    "tokenizer_obj.fit_on_texts(test_sentences)\n",
    "seq_test = tokenizer_obj.texts_to_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OWfRg3Ul_T4O",
    "outputId": "5c267123-dd08-45b3-ac6d-c15c52600913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5768\n"
     ]
    }
   ],
   "source": [
    "longestsentence = max([len(s.split()) for s in X_train])\n",
    "longestsentence_test = max([len(s.split()) for s in X_test])\n",
    "longestone = max(longestsentence, longestsentence_test)\n",
    "print(longestone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162.46018922852983"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics \n",
    "length_train = sum([len(s.split()) for s in X_train])/len([len(s.split()) for s in X_train]) #average length of sentences\n",
    "length_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162.29489465448856"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_test = sum([len(s.split()) for s in X_test])/len([len(s.split()) for s in X_test]) #average length of sentences\n",
    "length_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQQXUjjT_T4T"
   },
   "outputs": [],
   "source": [
    "# for train\n",
    "padded = pad_sequences(seq,200)\n",
    "# for test\n",
    "padded_test = pad_sequences(seq_test,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-O2hr9QltU_"
   },
   "outputs": [],
   "source": [
    "# We flipped the input matrices vertically since the zeros were inserted in the beginning of the sentence, \n",
    "# which could harm the semantic meaning of the words.\n",
    "\n",
    "padded = np.flip(padded, axis=1) \n",
    "padded_test = np.flip(padded_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UAlb2Lg_T4W"
   },
   "source": [
    "# CREATING THE DICTIONARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LuxCCGWD_T4X"
   },
   "source": [
    "In order to build the dictionary for unique words (word_index) in our train set, we used tokenizer function from keras. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, we used 'Global Vectors for Word Representation' method as the embedding metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_YcesME_T4Y"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer_obj.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0w75-7gO_T4a",
    "outputId": "6c50ed34-d543-4fb7-e391-a7c4a917f303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8wD5YDku_T4e"
   },
   "outputs": [],
   "source": [
    "emb=100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, emb))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2P7fMlx_T4g"
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZBlTPwUm9dDD"
   },
   "source": [
    "We used keras package for building the neural network model. For building the embedding layer of the NN,  we used the size of the vocabulary (len(word_index) +1 ) for input dimension and a reasonable number of a vector space (emb = 100) for output dimension.In the beginning, we determined the input length as the number of words from the longest sentence (longestone), while the weights will be implemented from the embedding matrix constructed above. After running our model, we changed our input length to 200, to prevent our computers to crash (the average length is 162)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27WFGx2L_T4g"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            emb,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=200,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_2kQkwD19lSi"
   },
   "source": [
    "We set the epoch size to a small number due to practical reasons, since each epoch lasted a lot of time. While building the NN, we added dropout layers to avoid overfitting and GRU to solve the vanishing gradient problem which comes with a standard recurrent NN. We used softmax as our activation function, which normalizes the output in a non-linear fashion so that the sum of output for all classes is equals to 1. Thereby, calculated probabilities are in the range of 0 to 1.Since it is a multiclass problem, we picked \"categorical_crossentropy\" as a loss function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CGYef3VQLq8"
   },
   "source": [
    "As optimizer we used Adaptive Moment Estimation (Adam), which is an adaptive learning rate method and takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum. Thus, it is also more stable and less volatile over the training epochs compared to other optimizer such as SGD. That’s why we only inserted learning rate and decay but no extra momentum. \n",
    "\n",
    "We chose a comparatively large learning rate of 0.1 and a smaller decay of 0.01 to optimize the learning over the small number of epochs. If we chose a smaller learning rate, larger number of epochs would be more suitable and stable. However, we would like to have the less time-consuming model since fitting our model is already very long process itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YUGnuq4b_x-q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 200, 100)          13796700  \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 16)                5616      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 13,802,401\n",
      "Trainable params: 13,802,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model without dropout\n",
    "epochs = 3\n",
    "\n",
    "model=Sequential()\n",
    "model.add(embedding_layer) \n",
    "model.add(GRU(16))\n",
    "model.add(Dense(5, activation=\"softmax\")) \n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.adam(lr=0.1, decay=0.01), metrics=[\"accuracy\"]) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time constraint, we only ran 3 epochs at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRVcatnO_T4k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 82440 samples, validate on 27481 samples\n",
      "Epoch 1/3\n",
      "82440/82440 [==============================] - 580s 7ms/step - loss: 1.1497 - acc: 0.6147 - val_loss: 1.1513 - val_acc: 0.6141\n",
      "Epoch 2/3\n",
      "82440/82440 [==============================] - 561s 7ms/step - loss: 1.1279 - acc: 0.6168 - val_loss: 1.1485 - val_acc: 0.6142\n",
      "Epoch 3/3\n",
      "82440/82440 [==============================] - 570s 7ms/step - loss: 1.1191 - acc: 0.6180 - val_loss: 1.1518 - val_acc: 0.6121\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded , y_train , batch_size=64, epochs=epochs, validation_data = (padded_test, y_test), shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python_Files\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 200, 100)          13796700  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200, 100)          0         \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 16)                5616      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 13,802,401\n",
      "Trainable params: 13,802,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model with dropout\n",
    "epochs = 3\n",
    "\n",
    "model=Sequential()\n",
    "model.add(embedding_layer) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(GRU(16, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Dense(5, activation=\"softmax\")) \n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.adam(lr=0.1, decay=0.01), metrics=[\"accuracy\"]) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 82440 samples, validate on 27481 samples\n",
      "Epoch 1/3\n",
      "82440/82440 [==============================] - 616s 7ms/step - loss: 1.1542 - acc: 0.6153 - val_loss: 1.1519 - val_acc: 0.6152\n",
      "Epoch 2/3\n",
      "82440/82440 [==============================] - 610s 7ms/step - loss: 1.1290 - acc: 0.6165 - val_loss: 1.1509 - val_acc: 0.6149\n",
      "Epoch 3/3\n",
      "82440/82440 [==============================] - 612s 7ms/step - loss: 1.1142 - acc: 0.6172 - val_loss: 1.1582 - val_acc: 0.6148\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(padded , y_train , batch_size=64, epochs=epochs, validation_data = (padded_test, y_test), shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with dropout gave us approxiamately loss of 1.13 and validation accuracy (on test data) of 0.615. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBx2KNtI_T5D"
   },
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjCswj3op1oK"
   },
   "source": [
    "For our second model, we used Multiclass Logistic Regression.\n",
    "\n",
    "In each observation, it assigns coefficients like we do in regular linear regression. After running for all observations, it simply calculates the averages of the input's (here word's) coefficients. In order to represent the input dataset as Bag of words, we will use CountVectorizer and call it’s transform method. It converts input texts into sparse matrix of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzcNH954_T5E"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-mRYz8xpzcM"
   },
   "source": [
    "CountVectorizer takes lowercase (default),  stopwords,  tokenizer as its inputs.\n",
    "However, it is unable to process lemmatize function, so it had to be introduced additionally.\n",
    "We introduced LemmaTokenizer, which performs tokenize and lemmatize at the same time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fgxx5fpsT-Q"
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, text):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ti9NsYbMqsac"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words= {'english'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eiIJKcbwEMi"
   },
   "source": [
    "As running tokenizer, stop words, tokenizer and lemmatizer on such as big data, we saved the results by using pickle again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XOyRHu5M_T5F"
   },
   "outputs": [],
   "source": [
    "# vectorizer.fit(X_train_log)\n",
    "\n",
    "# X_train_log = vectorizer.transform(X_train_log)\n",
    "# X_test_log  = vectorizer.transform(X_test_log)\n",
    "\n",
    "#  filehandler_log = open('X_train_log.pkl',\"wb\")\n",
    "#  pickle.dump(X_train_log,filehandler_log)\n",
    "#  filehandler_log.close()\n",
    "\n",
    "#  filehandler_test_log = open('X_test_log.pkl',\"wb\")\n",
    "#  pickle.dump(X_test_log,filehandler_test_log)\n",
    "#  filehandler_test_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P58EOTO-yZGO"
   },
   "outputs": [],
   "source": [
    "with open('X_train_log.pkl','rb') as filehandler_log:\n",
    "    X_train_log = pickle.load(filehandler_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SnBeXwwbydIs"
   },
   "outputs": [],
   "source": [
    "with open('X_test_log.pkl','rb') as filehandler_test_log:\n",
    "    X_test_log = pickle.load(filehandler_test_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCw4ErhYr2ig"
   },
   "source": [
    "Using 'undersampling' for logistic regression led us have higher log_loss and lower accuracy than not using any method for sampling.\n",
    "So we avoided using 'undersampling' method here and trained our model with train data (X_train_log here), analyzing first in terms of accuracy then log_loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_0g4x31_T5M",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(solver=\"lbfgs\")\n",
    "classifier.fit(X_train_log, y_train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_OVH4ChH3J5-"
   },
   "source": [
    "Applying our model on train dataset produced an accuracy score 74% while on test dataset it is 67%.\n",
    "These accuracy rates are better than our RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sN_6tvrY22mV",
    "outputId": "1d778eb8-6d5b-460e-8bac-f50c71087d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7498544395924308\n"
     ]
    }
   ],
   "source": [
    "score_lr_train = classifier.score(X_train_log, y_train_log)\n",
    "\n",
    "print(\"Accuracy:\", score_lr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TauzQ3bI25RT",
    "outputId": "27ceb2cf-a9dd-42b2-d324-6f6ede9342ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6771223754594083\n"
     ]
    }
   ],
   "source": [
    "score_lr_test = classifier.score(X_test_log, y_test_log)\n",
    "\n",
    "print(\"Accuracy:\", score_lr_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6A9Ffw3Q3buW"
   },
   "source": [
    "Our main concern in this analysis is Log_loss value of our predictions. \n",
    "Log_loss value of train predictions is 0.67 while log_loss value of train predictions is 0.94. These log_loss values are better than our RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EltJ_M0tzuWS",
    "outputId": "f16a8a5f-7d6f-4cb8-e4fe-6c3c59c5e623"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6736884799113928"
      ]
     },
     "execution_count": 259,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict_proba(X_train_log)\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_train_log, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0iHkYKahx3H0",
    "outputId": "314110d1-1814-413b-9687-805ec8c5b2cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9422729160279202"
      ]
     },
     "execution_count": 257,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = classifier.predict_proba(X_test_log)\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test_log, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdsZLdT09dNN"
   },
   "source": [
    "## PREDICTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RQz2Uh746YKo"
   },
   "source": [
    "Our NN model by its nature, promises better analysis for 'Natural Language Processing' as it helps us to analyze the interactions of embedding layers and to run recurrent neural network which it is better for capturing semantic meaning of the texts. \n",
    "According to our results, we figured out that our Logistic Regression model is a better model than RNN model in terms of loss. \n",
    "In order to get predictions, we repeated every step of Logistic Regression on Known Data (MC2) and Unknown Data (MC2 Test). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJPIc23lTLwK"
   },
   "source": [
    "As we compared the results of our two models, we determined that logistic regression model performed better in terms of log loss function. That leaded us to set RNN model as benchmark for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rQ8itwOCVaa"
   },
   "outputs": [],
   "source": [
    "known = pd.read_csv(\"sample_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImZtJvafC-FY"
   },
   "outputs": [],
   "source": [
    "X_known = known['review_body']\n",
    "y_known = known['star_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNrySnw0Cbyr"
   },
   "outputs": [],
   "source": [
    "unknown = pd.read_csv(\"MC2test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XIiJsW5XDmo8"
   },
   "outputs": [],
   "source": [
    "X_unknown = unknown['review_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ACpoBG_pLyGm"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVg6J5p2L1th"
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, text):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SnaT0eELM3dq"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words= {'english'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upE7OoFoMV-G"
   },
   "outputs": [],
   "source": [
    "#vectorizer.fit(X_known)\n",
    "\n",
    "#X_known = vectorizer.transform(X_known)\n",
    "#X_unknown  = vectorizer.transform(X_unknown)\n",
    "\n",
    "# filehandler_known = open('X_known.pkl',\"wb\")\n",
    "# pickle.dump(X_known,filehandler_known)\n",
    "# filehandler_known.close()\n",
    "\n",
    "#filehandler_unknown = open('X_unknown.pkl',\"wb\")\n",
    "#pickle.dump(X_unknown,filehandler_unknown)\n",
    "#filehandler_unknown.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Any3N26GL9Ay"
   },
   "outputs": [],
   "source": [
    "with open('X_known.pkl','rb') as filehandler_known:\n",
    "    X_known = pickle.load(filehandler_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dy9SW8XTL_GE"
   },
   "outputs": [],
   "source": [
    "with open('X_unknown.pkl','rb') as filehandler_unknown:\n",
    "    X_unknown = pickle.load(filehandler_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5i4HKMsUMBqC"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(solver=\"lbfgs\")\n",
    "classifier.fit(X_known, y_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZuB8cSDMFBi"
   },
   "outputs": [],
   "source": [
    "#predictions = model.predict_classes(X_test, verbose=0)\n",
    "predictions = classifier.predict(X_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3QllKkQbUen9"
   },
   "outputs": [],
   "source": [
    "unknown['product_id'] = unknown['product_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_bYN5PmMXbC"
   },
   "outputs": [],
   "source": [
    "def write_predictions(predictions, fname):\n",
    "  pd.DataFrame({\"product_id\": unknown['product_id'], \"star_rating\": predictions}).to_csv(fname, index=False, header=True)\n",
    "\n",
    "write_predictions(predictions, \"MC2_EMA.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "21_13.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
